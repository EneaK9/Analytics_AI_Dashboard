[1mdiff --cc backend/app.py[m
[1mindex 8c6dab6,c1a77d4..0000000[m
[1m--- a/backend/app.py[m
[1m+++ b/backend/app.py[m
[36m@@@ -587,297 -552,80 +587,308 @@@[m [masync def create_client_superadmin[m
                          "schema_definition": {"type": "raw_data", "format": data_type}[m
                      }).execute()[m
                      [m
[31m -                    try:[m
[31m -                        from universal_data_parser import universal_parser[m
[31m -                        [m
[31m -                        # Parse ANY format to standardized JSON records[m
[31m -                        parsed_records = universal_parser.parse_to_json(raw_data, data_type)[m
[31m -                        [m
[31m -                        if parsed_records:[m
[31m -                            logger.info(f"üîÑ {data_type.upper()} parsed to {len(parsed_records)} JSON records")[m
[32m +                    from universal_data_parser import universal_parser[m
[32m +                    [m
[32m +                    # Parse ANY format to standardized JSON records[m
[32m +                    parsed_records = universal_parser.parse_to_json(raw_data, data_type)[m
[32m +                    [m
[32m +                    if parsed_records:[m
[32m +                        logger.info(f"üîÑ {data_type.upper()} parsed to {len(parsed_records)} JSON records")[m
[32m +                        all_parsed_records.extend(parsed_records)[m
[32m +                [m
[32m +                elif input_method == "upload" and files_to_process:[m
[32m +                    # Process multiple uploaded files[m
[32m +                    from universal_data_parser import universal_parser[m
[32m +                    [m
[32m +                    # Simple schema entry for uploaded files[m
[32m +                    db_client.table("client_schemas").insert({[m
[32m +                        "client_id": client_id,[m
[32m +                        "table_name": f"client_{client_id.replace('-', '_')}_data",[m
[32m +                        "data_type": data_type,[m
[32m +                        "schema_definition": {"type": "multi_file_upload", "format": data_type, "file_count": len(files_to_process)}[m
[32m +                    }).execute()[m
[32m +                    [m
[32m +                    for file_idx, file in enumerate(files_to_process):[m
[32m +                        try:[m
[32m +                            logger.info(f"üìÑ Processing file {file_idx + 1}/{len(files_to_process)}: {file.filename}")[m
                              [m
[31m-                             # üõ°Ô∏è ENHANCED: Safe file reading with detailed diagnostics[m
[31m-                             try:[m
[31m-                                 # Check file size first[m
[31m-                                 file_size = 0[m
[31m-                                 if hasattr(file, 'size') and file.size is not None:[m
[31m-                                     file_size = file.size[m
[31m-                                     logger.info(f"üìè File size: {file_size} bytes ({file_size / 1024 / 1024:.2f} MB)")[m
[32m+                             # DEDUP + BATCH INSERT for ALL formats with retry logic[m
[32m+                             if parsed_records:[m
[32m+                                 from database import get_db_manager[m
[32m+                                 manager = get_db_manager()[m
[32m+                                 # Remove metadata fields before storing[m
[32m+                                 clean_records = [[m
[32m+                                     {k: v for k, v in r.items() if not str(k).startswith('_')}[m
[32m+                                     for r in parsed_records[m
[32m+                                 ][m
[32m+                                 total_inserted = await manager.dedup_and_batch_insert_client_data([m
[32m+                                     f"client_{client_id.replace('-', '_')}_data",[m
[32m+                                     clean_records,[m
[32m+                                     client_id,[m
[32m+                                     dedup_scope="day",[m
[32m+                                 )[m
[32m+                                 logger.info([m
[32m+                                     f"üöÄ TOTAL {data_type.upper()}: {total_inserted} new rows inserted after dedup!"[m
[32m+                                 )[m
                                  [m
[31m -                                # üìä PERFORMANCE MONITORING for large datasets[m
[31m -                                if len(parsed_records) > 10000:  # Track performance for large uploads[m
[31m -                                    success_rate = (total_inserted/len(parsed_records)*100)[m
[31m -                                    logger.info(f"üìä LARGE DATASET PERFORMANCE REPORT:")[m
[31m -                                    logger.info(f"   üìã Dataset: {len(parsed_records)} total records")[m
[31m -                                    logger.info(f"   ‚úÖ Inserted: {total_inserted} records")[m
[31m -                                    logger.info(f"   üìà Success Rate: {success_rate:.1f}%")[m
[31m -                                    logger.info(f"   ‚è±Ô∏è  Processing: Optimized chunking with retry logic")[m
[31m -                                    logger.info(f"   üéØ Client: {email}")[m
[32m +                                # NO LIMITS - READ THE ENTIRE FILE[m
[32m +                                file_content = await file.read()[m
[32m +                                logger.info(f"‚úÖ Read entire file {file.filename}: {len(file_content)} bytes ({len(file_content) / 1024 / 1024:.2f} MB)")[m
[32m +                                [m
[32m +                                # Basic validation[m
[32m +                                if not file_content:[m
[32m +                                    logger.error(f"‚ùå File {file.filename} is empty or could not be read")[m
[32m +                                    continue[m
                                      [m
[31m -                                    # Record performance metrics for monitoring[m
[31m -                                    try:[m
[31m -                                        db_client.table("performance_metrics").insert({[m
[31m -                                            "client_id": client_id,[m
[31m -                                            "operation_type": "large_csv_upload",[m
[31m -                                            "total_records": len(parsed_records),[m
[31m -                                            "records_inserted": total_inserted,[m
[31m -                                            "success_rate": round(success_rate, 2),[m
[31m -                                            "data_type": data_type,[m
[31m -                                            "timestamp": datetime.utcnow().isoformat()[m
[31m -                                        }).execute()[m
[31m -                                        logger.info(f"üìä Performance metrics recorded for {email}")[m
[31m -                                    except Exception as metrics_error:[m
[31m -                                        logger.warning(f"‚ö†Ô∏è Could not record performance metrics: {metrics_error}")[m
[32m +                            except Exception as read_error:[m
[32m +                                logger.error(f"‚ùå Failed to read file {file.filename}: {read_error}")[m
[32m +                                logger.error(f"üîß File read error type: {type(read_error).__name__}")[m
                                  [m
[31m -                        else:[m
[31m -                            raise ValueError(f"{data_type.upper()} parsing returned no records")[m
[32m +                                # Try to create a placeholder record for failed file reads[m
[32m +                                failed_read_record = {[m
[32m +                                    'filename': file.filename,[m
[32m +                                    'error': f'File read failed: {str(read_error)[:200]}',[m
[32m +                                    'error_type': 'file_read_error',[m
[32m +                                    'status': 'read_failed',[m
[32m +                                    '_source_file': file.filename,[m
[32m +                                    '_read_failed': True[m
[32m +                                }[m
[32m +                                all_parsed_records.append(failed_read_record)[m
[32m +                                logger.info(f"‚ö†Ô∏è Added placeholder record for failed file read: {file.filename}")[m
[32m +                                continue[m
[32m +                            [m
[32m +                            # Determine file format based on extension or data_type[m
[32m +                            file_extension = file.filename.split('.')[-1].lower()[m
[32m +                            file_format = data_type[m
[32m +                            [m
[32m +                            # Auto-detect format from file extension if needed[m
[32m +                            if file_extension in ['xlsx', 'xls']:[m
[32m +                                file_format = 'excel'[m
[32m +                            elif file_extension == 'bak':[m
[32m +                                file_format = 'bak'[m
[32m +                                # Special handling for database backup files[m
[32m +                                if any(db_indicator in file.filename.lower() for db_indicator in ['db', 'database', 'sql', 'mysql', 'postgres', 'oracle', 'mssql']):[m
[32m +                                    logger.info(f"üóÉÔ∏è File {file.filename} appears to be a database backup")[m
[32m +                            elif file_extension == 'csv':[m
[32m +                                file_format = 'csv'[m
[32m +                            elif file_extension == 'json':[m
[32m +                                file_format = 'json'[m
[32m +                            elif file_extension == 'xml':[m
[32m +                                file_format = 'xml'[m
                              [m
[31m -                    except Exception as parse_error:[m
[31m -                        logger.error(f"‚ùå {data_type.upper()} parsing failed: {parse_error}")[m
[31m -                        # Fallback: store as raw text (old behavior)[m
[31m -                        db_client.table("client_data").insert({[m
[32m +                            # Handle Excel files differently (binary content)[m
[32m +                            if file_format == 'excel':[m
[32m +                                # For Excel files, we need to handle binary content[m
[32m +                                try:[m
[32m +                                    import pandas as pd[m
[32m +                                    from io import BytesIO[m
[32m +                                    from datetime import datetime[m
[32m +                                    df = pd.read_excel(BytesIO(file_content), engine='openpyxl')[m
[32m +                                    [m
[32m +                                    # Convert to records with JSON serialization handling[m
[32m +                                    parsed_records = [][m
[32m +                                    for i, row in df.iterrows():[m
[32m +                                        record = {}[m
[32m +                                        for col, value in row.items():[m
[32m +                                            clean_col = str(col).strip().replace(' ', '_').replace('-', '_')[m
[32m +                                            clean_col = ''.join(c for c in clean_col if c.isalnum() or c == '_')[m
[32m +                                            [m
[32m +                                            if pd.isna(value):[m
[32m +                                                record[clean_col] = None[m
[32m +                                            else:[m
[32m +                                                # Handle different data types for JSON serialization[m
[32m +                                                if isinstance(value, pd.Timestamp):[m
[32m +                                                    # Convert pandas Timestamp to ISO string[m
[32m +                                                    record[clean_col] = value.isoformat()[m
[32m +                                                elif hasattr(value, 'isoformat'):[m
[32m +                                                    # Handle other datetime objects[m
[32m +                                                    record[clean_col] = value.isoformat()[m
[32m +                                                elif isinstance(value, (pd.Int64Dtype, pd.Float64Dtype)):[m
[32m +                                                    # Handle pandas nullable integers/floats[m
[32m +                                                    record[clean_col] = float(value) if pd.notna(value) else None[m
[32m +                                                elif hasattr(value, 'item'):[m
[32m +                                                    # Handle numpy scalars[m
[32m +                                                    record[clean_col] = value.item()[m
[32m +                                                else:[m
[32m +                                                    # Convert to string for complex objects, keep primitives as-is[m
[32m +                                                    if isinstance(value, (str, int, float, bool)):[m
[32m +                                                        record[clean_col] = value[m
[32m +                                                    else:[m
[32m +                                                        record[clean_col] = str(value)[m
[32m +                                        [m
[32m +                                        record['_row_number'] = i + 1[m
[32m +                                        record['_source_format'] = 'excel'[m
[32m +                                        record['_source_file'] = file.filename[m
[32m +                                        parsed_records.append(record)[m
[32m +                                    [m
[32m +                                    logger.info(f"‚úÖ Excel file '{file.filename}' parsed: {len(parsed_records)} records")[m
[32m +                                    all_parsed_records.extend(parsed_records)[m
[32m +                                    [m
[32m +                                except ImportError:[m
[32m +                                    logger.error("‚ùå pandas/openpyxl not available for Excel parsing")[m
[32m +                                    continue[m
[32m +                                except Exception as e:[m
[32m +                                    logger.error(f"‚ùå Excel parsing failed for {file.filename}: {e}")[m
[32m +                                    continue[m
[32m +                            else:[m
[32m +                                # For text-based files and binary files that need special handling[m
[32m +                                try:[m
[32m +                                    # Try to decode as UTF-8 first[m
[32m +                                    raw_data = file_content.decode('utf-8')[m
[32m +                                    parsed_records = universal_parser.parse_to_json(raw_data, file_format)[m
[32m +                                    [m
[32m +                                    if parsed_records:[m
[32m +                                        # Add file metadata[m
[32m +                                        for record in parsed_records:[m
[32m +                                            if isinstance(record, dict):[m
[32m +                                                record['_source_file'] = file.filename[m
[32m +                                        [m
[32m +                                        logger.info(f"‚úÖ File '{file.filename}' parsed: {len(parsed_records)} records")[m
[32m +                                        all_parsed_records.extend(parsed_records)[m
[32m +                                    else:[m
[32m +                                        logger.warning(f"‚ö†Ô∏è No records parsed from file: {file.filename}")[m
[32m +                                        [m
[32m +                                except UnicodeDecodeError:[m
[32m +                                    # Handle binary files or files with different encodings[m
[32m +                                    logger.warning(f"‚ö†Ô∏è File {file.filename} is not UTF-8, trying alternative approaches...")[m
[32m +                                    [m
[32m +                                    # For .bak files, try different approaches[m
[32m +                                    if file_format == 'bak':[m
[32m +                                        try:[m
[32m +                                            # Try different encodings for .bak files[m
[32m +                                            for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:[m
[32m +                                                try:[m
[32m +                                                    raw_data = file_content.decode(encoding)[m
[32m +                                                    logger.info(f"‚úÖ Successfully decoded {file.filename} using {encoding} encoding")[m
[32m +                                                    parsed_records = universal_parser.parse_to_json(raw_data, file_format)[m
[32m +                                                    [m
[32m +                                                    if parsed_records:[m
[32m +                                                        # Add file metadata[m
[32m +                                                        for record in parsed_records:[m
[32m +                                                            if isinstance(record, dict):[m
[32m +                                                                record['_source_file'] = file.filename[m
[32m +                                                                record['_encoding_used'] = encoding[m
[32m +                                                        [m
[32m +                                                        logger.info(f"‚úÖ File '{file.filename}' parsed with {encoding}: {len(parsed_records)} records")[m
[32m +                                                        all_parsed_records.extend(parsed_records)[m
[32m +                                                        break[m
[32m +                                                except (UnicodeDecodeError, UnicodeError):[m
[32m +                                                    continue[m
[32m +                                            else:[m
[32m +                                                # If all encodings fail, treat as binary and create a metadata record[m
[32m +                                                logger.warning(f"‚ö†Ô∏è Could not decode {file.filename} with any encoding, storing as binary metadata")[m
[32m +                                                binary_record = {[m
[32m +                                                    'filename': file.filename,[m
[32m +                                                    'file_size_bytes': len(file_content),[m
[32m +                                                    'file_type': 'binary_backup',[m
[32m +                                                    'content_preview': str(file_content[:100]) if len(file_content) > 0 else 'empty',[m
[32m +                                                    '_source_file': file.filename,[m
[32m +                                                    '_binary_file': True[m
[32m +                                                }[m
[32m +                                                all_parsed_records.append(binary_record)[m
[32m +                                                logger.info(f"‚úÖ Binary file '{file.filename}' stored as metadata record")[m
[32m +                                                [m
[32m +                                        except Exception as e:[m
[32m +                                            logger.error(f"‚ùå Error processing .bak file {file.filename}: {e}")[m
[32m +                                            continue[m
[32m +                                    else:[m
[32m +                                        # For other file types, log the error and skip[m
[32m +                                        logger.error(f"‚ùå Cannot decode file {file.filename} as UTF-8 and no alternative handling available")[m
[32m +                                        continue[m
[32m +                                        [m
[32m +                                except Exception as e:[m
[32m +                                    logger.error(f"‚ùå Error parsing file {file.filename}: {e}")[m
[32m +                                    continue[m
[32m +                        [m
[32m +                        except Exception as e:[m
[32m +                            logger.error(f"‚ùå Error processing file {file.filename}: {e}")[m
[32m +                            continue[m
[32m +                [m
[32m +                # Process all collected records[m
[32m +                if all_parsed_records:[m
[32m +                    logger.info(f"üîÑ Processing {len(all_parsed_records)} total records from all sources")[m
[32m +                    [m
[32m +                    # BATCH INSERT - Same logic for ALL formats![m
[32m +                    batch_rows = [][m
[32m +                    for record in all_parsed_records:[m
[32m +                        # Remove metadata fields before storing[m
[32m +                        clean_record = {k: v for k, v in record.items() if not k.startswith('_')}[m
[32m +                        batch_rows.append({[m
                              "client_id": client_id,[m
                              "table_name": f"client_{client_id.replace('-', '_')}_data",[m
[31m -                            "data": {"raw_content": raw_data, "type": data_type}[m
[31m -                        }).execute()[m
[31m -                        logger.info(f"‚ö†Ô∏è  {data_type.upper()} stored as fallback raw text")[m
[31m -                    [m
[31m -                    logger.info(f"‚ö° Data stored DIRECTLY for {email} - NOW TRIGGER AI!")[m
[32m +                            "data": clean_record  # Store as JSON object[m
[32m +                        })[m
                      [m
[31m -                    # üéØ NOW TRIGGER AI DASHBOARD GENERATION AFTER DATA IS SAFELY STORED[m
[31m -                    try:[m
[31m -                        logger.info(f"üöÄ NOW triggering AI dashboard generation for {email} (data is ready!)")[m
[32m +                    # OPTIMIZED BATCH INSERT for ALL formats with retry logic[m
[32m +                    if batch_rows:[m
[32m +                        total_inserted = await improved_batch_insert(db_client, batch_rows, data_type)[m
[32m +                        logger.info(f"üöÄ TOTAL RECORDS: {total_inserted} rows inserted successfully!")[m
                          [m
[31m -                        # IMPROVED: Better error handling and more robust generation[m
[31m -                        async def robust_dashboard_generation():[m
[32m +                        # üìä PERFORMANCE MONITORING for large datasets[m
[32m +                        if len(all_parsed_records) > 10000:  # Track performance for large uploads[m
[32m +                            success_rate = (total_inserted/len(all_parsed_records)*100)[m
[32m +                            logger.info(f"üìä LARGE DATASET PERFORMANCE REPORT:")[m
[32m +                            logger.info(f"   üìã Dataset: {len(all_parsed_records)} total records")[m
[32m +                            logger.info(f"   ‚úÖ Inserted: {total_inserted} records")[m
[32m +                            logger.info(f"   üìà Success Rate: {success_rate:.1f}%")[m
[32m +                            logger.info(f"   ‚è±Ô∏è  Processing: Multi-file upload with retry logic")[m
[32m +                            logger.info(f"   üéØ Client: {email}")[m
[32m +                            [m
[32m +                            # Record performance metrics for monitoring[m
[32m +                            try:[m
[32m +                                db_client.table("performance_metrics").insert({[m
[32m +                                    "client_id": client_id,[m
[32m +                                    "operation_type": "multi_file_upload",[m
[32m +                                    "total_records": len(all_parsed_records),[m
[32m +                                    "records_inserted": total_inserted,[m
[32m +                                    "success_rate": round(success_rate, 2),[m
[32m +                                    "data_type": data_type,[m
[32m +                                    "file_count": len(files_to_process) if input_method == "upload" else 1,[m
[32m +                                    "timestamp": datetime.utcnow().isoformat()[m
[32m +                                }).execute()[m
[32m +                                logger.info(f"üìä Performance metrics recorded for {email}")[m
[32m +                            except Exception as metrics_error:[m
[32m +                                logger.warning(f"‚ö†Ô∏è Could not record performance metrics: {metrics_error}")[m
[32m +                    else:[m
[32m +                        raise ValueError("No records parsed from any source")[m
[32m +                [m
[32m +            except Exception as parse_error:[m
[32m +                logger.error(f"‚ùå Data parsing failed: {parse_error}")[m
[32m +                [m
[32m +                # üõ°Ô∏è ENHANCED: Store fallback data for both paste AND file uploads[m
[32m +                # This ensures dashboard generation can still proceed with error info[m
[32m +                fallback_data = None[m
[32m +                [m
[32m +                if input_method == "paste" and data_content:[m
[32m +                    fallback_data = {"raw_content": data_content, "type": data_type, "parse_error": str(parse_error)[:200]}[m
[32m +                elif input_method == "upload" and files_to_process:[m
[32m +                    # For file uploads, create a summary record with error info[m
[32m +                    first_file = files_to_process[0][m
[32m +                    fallback_data = {[m
[32m +                        "filename": first_file.filename if first_file.filename else 'unknown_file',[m
[32m +                        "file_size": first_file.size if hasattr(first_file, 'size') else 0,[m
[32m +                        "parse_error": str(parse_error)[:200],[m
[32m +                        "type": data_type,[m
[32m +                        "status": "parsing_failed",[m
[32m +                        "error_type": "unicode_error" if "unicode" in str(parse_error).lower() else "parsing_error"[m
[32m +                    }[m
[32m +                [m
[32m +                if fallback_data:[m
[32m +                    db_client.table("client_data").insert({[m
[32m +                        "client_id": client_id,[m
[32m +                        "table_name": f"client_{client_id.replace('-', '_')}_data",[m
[32m +                        "data": fallback_data[m
[32m +                    }).execute()[m
[32m +                    logger.info(f"‚ö†Ô∏è Fallback data stored for {input_method} method")[m
[32m +            [m
[32m +            logger.info(f"‚ö° Data stored DIRECTLY for {email} - NOW TRIGGER AI!")[m
[32m +            [m
[32m +            # üéØ NOW TRIGGER AI DASHBOARD GENERATION AFTER DATA IS SAFELY STORED[m
[32m +            try:[m
[32m +                logger.info(f"üöÄ NOW triggering AI dashboard generation for {email} (data is ready!)")[m
[32m +                [m
[32m +                # IMPROVED: Better error handling and more robust generation[m
[32m +                async def robust_dashboard_generation():[m
                              """Robust async dashboard generation with detailed error logging"""[m
                              try:[m
                                  # Wait a moment to ensure data is committed[m
